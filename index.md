## Welcome to RLDS Pages
  
Reinforcement Learning with Decoupled State Representation based on the Robot Conditional Variational Autoencoder

**Authors:** Kun Dong#, Yongle Luo#, Shan Fang, Yuxin Wang, Lili Zhao, Qiang Zhang, Erkang Cheng, Zhiyong Sun, and Bo Songâˆ—

**Abstract:** Deep reinforcement learning (DRL) has advanced the robotic control in the manipulation task especially for providing an alternative solution to design an end-to-end control strategy with the raw image as the input directly. Although these images come with more knowledge about the environment, the visual information can hardly be used directly for the DRL because of the high sample complexity, which makes the policy have to achieve the representation learning and task learning simultaneously. Prior attempts, such as variational autoencoder (VAE) based DRL algorithms try to solve it by learning a visual representation model, which encodes the entire image into a low-dimension vector that includes the information of both the robot and the target. In this study, a reinforcement learning with decoupled state (RLDS) method is proposed to decouple the target and the robot information to increase the learning efficiency and effectiveness. The experimental results illustrate that the proposed method learns faster and achieves better final performance compared with previous methods in two typical robot manipulation tasks.

[https://rldscvae.github.io/](https://rldscvae.github.io/)

![GIF](https://github.com/RLDSCVAE/RLDSCVAE.github.io/blob/main/RLDS-last-2m.gif)

<img src="https://github.com/RLDSCVAE/RLDSCVAE.github.io/blob/main/RLDS-last-2m.gif" alt="zigzag" />

<img src="https://github.com/RLDSCVAE/RLDSCVAE.github.io/blob/main/RLDS-last-2m.gif" width=1000>
